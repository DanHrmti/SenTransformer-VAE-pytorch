{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Instructions\n",
        "This notebook includes all of the code for the Sentence Transformer-VAE. <br>\n",
        "In order to reproduce the results of this model you must:\n",
        "- First choose the [dataset](#-Model-Definition-and-Training) and the [hyperparameters](#-Hyperparameters).\n",
        "- Then run all of the cells up to the \"[Training](#-Training)\" section (not including).\n",
        "- To train the model you can either do so by running the [training loop cell](#-Training) in this notebook or using the `main.py` provided in the [GitHub repository](https://github.com/DanHrmti/SenTransformer-VAE-pytorch).\n",
        "- After acquiring a model checkpoint, load it by following the instructions in the \"[Load Model from Checkpoint](#-Load-Model-from-Checkpoint)\" section.\n",
        "- Follow the instructions and run the relevant cells in each of the results sections:\n",
        "    - [Test Perplexity](#-Test-Perplexity)\n",
        "    - [Sentence Generation](#-Sentence-Generation)\n",
        "    - [Sentence Reconstruction](#-Sentence-Reconstruction)\n",
        "    - [Interpolation between Sentences in the Latent Space](#-Interpolation-between-Sentences-in-the-Latent-Space)\n",
        "    - [Encoded Variance Analysis](#-Encoded-Variance-Analysis)"
      ],
      "metadata": {
        "id": "SS4NdhMBdF6E"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-etxlqMROLw"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBHkABuWQxwg"
      },
      "outputs": [],
      "source": [
        "# general\n",
        "import time\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
        "from torch.utils.data import DataLoader\n",
        "import torchtext\n",
        "import torchtext.legacy.data as data\n",
        "import torchtext.legacy.datasets as datasets\n",
        "\n",
        "# constant seed\n",
        "seed = 41\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "print(\"Importing finished\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Le223hihWQDS"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIENEaHYuKt-"
      },
      "outputs": [],
      "source": [
        "def data2sentences(data, name='PTB'):\n",
        "    sentence_data = []\n",
        "    cur_sentence = []\n",
        "    text = data.examples[0].text\n",
        "\n",
        "    if name == 'PTB':\n",
        "        for i, word in enumerate(text):\n",
        "            if word == '<eos>':\n",
        "                sentence_data.append(cur_sentence)\n",
        "                cur_sentence = []\n",
        "            else:\n",
        "                cur_sentence.append(word)\n",
        "    \n",
        "    if name == 'WikiText2': # TODO: better parsing into sentences\n",
        "        for i, word in enumerate(text):\n",
        "            if ((word == '.' and (len(text[i+1]) != 1 or text[i+1] in ['a', 'i']) \n",
        "                             and (len(text[i-1]) != 1 or text[i-1] in [')', ']', '}']))\n",
        "                             and (text[i-1] not in ['mr', 'mrs', 'dr', 'jr', 'prof', 'op'])\n",
        "                or word in ['<eos>', '?', '!']):\n",
        "                sentence_data.append(cur_sentence)\n",
        "                cur_sentence = []\n",
        "            elif word != \"=\":\n",
        "                cur_sentence.append(word)\n",
        "    \n",
        "    return sentence_data\n",
        "\n",
        "def preprocess_text(name, dataset, textField, max_len=50, plot=False):\n",
        "    sentences_raw = data2sentences(dataset, name)\n",
        "     \n",
        "    #################\n",
        "    if plot:\n",
        "        sentence_length = [len(t) for t in sentences_raw]\n",
        "        plt.hist(sentence_length, bins=84)\n",
        "        plt.title(\"Sentence Length Distribution - Before Filter\")\n",
        "        plt.show()\n",
        "    #################\n",
        "    \n",
        "    sentences_filtered = []\n",
        "    for i in range(len(sentences_raw)):\n",
        "      if 3 <= len(sentences_raw[i]) <= max_len:\n",
        "        sentences_filtered.append(sentences_raw[i])\n",
        "    \n",
        "    #################\n",
        "    if plot:\n",
        "        print(\"\\n\")\n",
        "        sentence_length = [len(t) for t in sentences_filtered]\n",
        "        plt.hist(sentence_length, bins=43)\n",
        "        plt.title(\"Sentence Length Distribution - After Filter\")\n",
        "        plt.show()\n",
        "    #################\n",
        "    \n",
        "    sentences = textField.pad(sentences_filtered)\n",
        "    sentences = textField.numericalize(sentences)\n",
        "    return sentences[:, 1:]\n",
        "\n",
        "def pad_token_sen(token_sen, max_sen_len, pad_idx):\n",
        "    pad_add = torch.full((1, max_sen_len - len(token_sen) + 2), pad_idx).to(device)\n",
        "    token_sen = token_sen.transpose(0,1)\n",
        "    out = torch.cat((token_sen, pad_add), 1)\n",
        "    out = out.transpose(1,0)\n",
        "    return out\n",
        "\n",
        "def create_masks(batch, pad_token):\n",
        "    pad_mask = (batch == pad_token).transpose(0, 1)\n",
        "\n",
        "    seq_len = batch.shape[0]\n",
        "    mask = (torch.triu(torch.ones((seq_len, seq_len), device=batch.device)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "\n",
        "    return pad_mask, mask\n",
        "\n",
        "def calc_kl(mu, logvar, reduce='mean'):\n",
        "    kl = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp()).sum(1)\n",
        "    if reduce == 'sum':\n",
        "        kl = torch.sum(kl)\n",
        "    elif reduce == 'mean':\n",
        "        kl = torch.mean(kl)\n",
        "    return kl\n",
        "\n",
        "def cyc_beta_scheduler(epochs, warmup_epochs, beta_min, beta_max, period, ratio):\n",
        "    beta_warmup = np.ones(warmup_epochs) * beta_min\n",
        "    beta_cyc = np.ones(epochs - warmup_epochs) * beta_max\n",
        "    n_cycle = int(np.floor((epochs - warmup_epochs)/period))\n",
        "    step = (beta_max - beta_min)/(period * ratio)\n",
        "    for c in range(n_cycle):\n",
        "        curr_beta, i = beta_min, 0\n",
        "        while curr_beta <= beta_max and (int(i + c*period) < epochs - warmup_epochs):\n",
        "              beta_cyc[int(i + c*period)] = curr_beta\n",
        "              curr_beta += step\n",
        "              i += 1\n",
        "    beta = np.concatenate((beta_warmup, beta_cyc), axis=0)\n",
        "    return beta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHbncC6ZRiDy"
      },
      "source": [
        "## TransformerVAE Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9131eKLRqLF"
      },
      "outputs": [],
      "source": [
        "# Building Blocks\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=50, dropout=0.1):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class Word2SentenceEmbedding(nn.Module):\n",
        "    def __init__(self, hdim):\n",
        "        super(Word2SentenceEmbedding, self).__init__()\n",
        "        self.dense = nn.Linear(hdim, hdim)\n",
        "        self.activation = nn.Tanh()\n",
        " \n",
        "    def forward(self, hidden_states):\n",
        "        # take the hidden state corresponding to <sos> token\n",
        "        first_token_tensor = hidden_states[0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output\n",
        "\n",
        "\n",
        "# Encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, ntokens, e_dim=200, z_dim=32, nheads=4, nTlayers=4, ff_dim=400, pad_idx=1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.e_dim = e_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(ntokens, e_dim, padding_idx=pad_idx)\n",
        "        self.pos_encoding = PositionalEncoding(e_dim)\n",
        "        encoder_layers = TransformerEncoderLayer(d_model=e_dim, nhead=nheads, dim_feedforward=ff_dim, dropout=0.2)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layer=encoder_layers, num_layers=nTlayers)\n",
        "        self.word2sen_hidden = Word2SentenceEmbedding(hdim=e_dim)\n",
        "        self.hid2latparams = nn.Linear(e_dim, 2*z_dim)\n",
        "\n",
        "    def forward(self, sentences, pad_mask):\n",
        "        embedded = self.embedding(sentences) * math.sqrt(self.e_dim)\n",
        "        embedded = self.pos_encoding(embedded)\n",
        "        hidden = self.transformer_encoder(embedded, src_key_padding_mask=pad_mask)\n",
        "        hidden = self.word2sen_hidden(hidden)\n",
        "        y = self.hid2latparams(hidden)\n",
        "        mu, logvar = y.chunk(2, dim=1)\n",
        "        return mu, logvar\n",
        "\n",
        "# Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, ntokens, e_dim=256, z_dim=32, nheads=4, nTlayers=4, ff_dim=1024, pad_idx=1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.e_dim = e_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(ntokens, e_dim, padding_idx=pad_idx)\n",
        "        self.pos_encoding = PositionalEncoding(e_dim)\n",
        "        self.lat2hid = nn.Linear(z_dim, e_dim)\n",
        "        decoder_layers = TransformerDecoderLayer(d_model=e_dim, nhead=nheads, dim_feedforward=ff_dim, dropout=0.2)\n",
        "        self.transformer_decoder = TransformerDecoder(decoder_layer=decoder_layers, num_layers=nTlayers)\n",
        "        self.hid2logits = nn.Linear(e_dim, ntokens)\n",
        "\n",
        "    def forward(self, z, sentences, tgt_mask, tgt_pad_mask):\n",
        "        memories = self.lat2hid(z)\n",
        "        memories = memories.unsqueeze(0)\n",
        "        embedded_targets = self.embedding(sentences) * math.sqrt(self.e_dim)\n",
        "        embedded_targets = self.pos_encoding(embedded_targets)\n",
        "        \n",
        "        hidden = self.transformer_decoder(embedded_targets, memories, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_pad_mask)\n",
        "        logits = self.hid2logits(hidden)\n",
        "        return logits\n",
        "\n",
        "# TransformerVAE\n",
        "class TransformerVAE(nn.Module):\n",
        "    def __init__(self, ntokens, e_dim=256, z_dim=32, nheads=4, ff_dim=1024, nTElayers=4, nTDlayers=4, pad_idx=1):\n",
        "        super(TransformerVAE, self).__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.encoder = Encoder(ntokens, e_dim, z_dim, nheads, nTElayers, ff_dim, pad_idx)\n",
        "        self.decoder = Decoder(ntokens, e_dim, z_dim, nheads, nTDlayers, ff_dim, pad_idx)\n",
        "\n",
        "    def forward(self, sentences, pad_mask, tgt_mask):\n",
        "        mu, logvar = self.encoder(sentences, pad_mask)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        logits = self.decoder(z, sentences, tgt_mask, pad_mask)\n",
        "        return mu, logvar, logits\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        # apply the reparameterization trick\n",
        "        device = mu.device\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std).to(device)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def encode(self, sentence, pad_token=1):\n",
        "        sentence = sentence.to(device)\n",
        "        pad_mask = (sentence == pad_token).transpose(0, 1)\n",
        "        mu, logvar = self.encoder(sentence, pad_mask)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return z\n",
        "    \n",
        "    def generate(self, z=None, device=\"cpu\", max_sen_len=50, sos_idx=2, eos_idx=3, policy='greedy', k=10):\n",
        "        if z is None: # sample z ~ N(0,I)\n",
        "            z = torch.randn(self.z_dim).unsqueeze(0).to(device)\n",
        "\n",
        "        next_word_probs = []\n",
        "\n",
        "        # first target is <sos>\n",
        "        sen = torch.ones(1, 1).fill_(sos_idx).type(torch.long).to(device)\n",
        "        for i in range(max_sen_len-1):\n",
        "            # create mask\n",
        "            mask = (torch.triu(torch.ones((len(sen), len(sen)), device=device)) == 1).transpose(0, 1)\n",
        "            mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "            # decode\n",
        "            out = self.decoder(z, sen, tgt_mask=mask, tgt_pad_mask=None)\n",
        "            next_word_prob = out[-1].squeeze()\n",
        "\n",
        "            # TODO: implement 'topP' and 'beam_search' policies\n",
        "\n",
        "            if policy == 'topK':\n",
        "                topk_values, topk_tokens = torch.topk(next_word_prob, k)\n",
        "                topk_probs = torch.exp(topk_values) / torch.sum(torch.exp(topk_values)) # softmax\n",
        "                idx = torch.multinomial(topk_probs, 1).item()\n",
        "                next_word = topk_tokens[idx].item()\n",
        "\n",
        "            elif policy == 'multinomial':\n",
        "                next_word_prob_softmax = torch.exp(next_word_prob) / torch.sum(torch.exp(next_word_prob)) # softmax\n",
        "                next_word = torch.multinomial(next_word_prob_softmax, 1)\n",
        "                next_word = next_word.item()\n",
        "\n",
        "            else: # 'greedy'\n",
        "                next_word = torch.argmax(next_word_prob)\n",
        "                next_word = next_word.item()\n",
        "              \n",
        "            # add decoded word to sentence\n",
        "            sen = torch.cat([sen, torch.ones(1, 1).fill_(next_word).type(torch.long).to(device)], dim=0)\n",
        "            next_word_probs.append(next_word_prob)\n",
        "            if next_word == eos_idx:\n",
        "                break\n",
        "\n",
        "\n",
        "        return sen, next_word_probs\n",
        "\n",
        "    def reconstruct(self, sen_orig, device=\"cpu\", max_sen_len=50, pad_idx=1, sos_idx=2, eos_idx=3, policy='greedy', k=4):\n",
        "        # encode original sentence to latent space representation\n",
        "        z = self.encode(sen_orig, pad_idx)\n",
        "        # decode from latent space\n",
        "        sen_rec, _ = self.generate(z, device, max_sen_len, sos_idx, eos_idx, policy, k)\n",
        "        return sen_rec\n",
        "\n",
        "    def interpolate(self, sen_1=None, sen_2=None, intervals=10, device=\"cpu\", max_sen_len=50, pad_idx=1, sos_idx=2, eos_idx=3, policy='greedy', k=4):\n",
        "        if sen_1 is not None: # encode\n",
        "            z_1 = self.encode(sen_1, pad_idx)\n",
        "        else: # sample z ~ N(0,I)\n",
        "            z_1 = torch.randn(self.z_dim).unsqueeze(0).to(device)\n",
        "        \n",
        "        if sen_2 is not None: # encode\n",
        "            z_2 = self.encode(sen_2, pad_idx)\n",
        "        else: # sample z ~ N(0,I)\n",
        "            z_2 = torch.randn(self.z_dim).unsqueeze(0).to(device)\n",
        "\n",
        "        sentences = []\n",
        "        for i in range(intervals+1):\n",
        "            t = i / intervals\n",
        "            z = z_1 * (1-t) + z_2 * t\n",
        "            sen, _ = self.generate(z, device, max_sen_len, sos_idx, eos_idx, policy, k)\n",
        "            sentences.append(sen.squeeze())\n",
        "        \n",
        "        return sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMl_SCu3a3QJ"
      },
      "source": [
        "## Model Definition and Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose Dataset: ['PTB', 'WikiText2']\n",
        "dataset = 'PTB'"
      ],
      "metadata": {
        "id": "ljlP0pcjEMLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm5nzy5hDFGw"
      },
      "source": [
        "### Preparing the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YX8bwcZWDKPO"
      },
      "outputs": [],
      "source": [
        "# Initialize tokenizer and text field\n",
        "tokenizer = data.utils.get_tokenizer(\"basic_english\")\n",
        "textField = data.Field(sequential=True, init_token='<sos>', eos_token='<eos>', lower=True, tokenize=tokenizer)\n",
        "\n",
        "# Load and split dataset\n",
        "if dataset == 'PTB': # PennTreebank\n",
        "    train_set, valid_set, test_set = datasets.PennTreebank.splits(textField)\n",
        "elif dataset == 'WikiText2': # WikiText-2\n",
        "    train_set, valid_set, test_set = datasets.WikiText2.splits(textField)\n",
        "else:\n",
        "    raise NotImplementedError(\"Dataset is not supported\")\n",
        "\n",
        "# Build a vocabulary\n",
        "# textField.build_vocab(train_set, valid_set, test_set)\n",
        "textField.build_vocab(train_set)\n",
        "pad_idx = textField.vocab.stoi['<pad>']\n",
        "sos_idx = textField.vocab.stoi['<sos>']\n",
        "eos_idx = textField.vocab.stoi['<eos>']\n",
        "len_vocab = len(textField.vocab)\n",
        "print(f'Size of vocabulary: {len_vocab}\\n')\n",
        "\n",
        "# Pre-process data into tokenized padded sentences\n",
        "max_sen_len = 45\n",
        "sentence_data = preprocess_text(dataset, train_set, textField, max_len=max_sen_len, plot=True)\n",
        "num_sentences = sentence_data.shape[1]\n",
        "print(f'\\nNumber of sentences in train set: {num_sentences}')\n",
        "\n",
        "sentence_data_validation = preprocess_text(dataset, valid_set, textField, max_len=max_sen_len)\n",
        "num_validation_sentences = sentence_data_validation.shape[1]\n",
        "print(f'\\nNumber of sentences in validation set: {num_validation_sentences}')\n",
        "\n",
        "sentence_data_test = preprocess_text(dataset, test_set, textField, max_len=max_sen_len)\n",
        "num_test_sentences = sentence_data_test.shape[1]\n",
        "print(f'\\nNumber of sentences in test set: {num_test_sentences}\\n\\n')\n",
        "\n",
        "# Print sentence samples from dataset\n",
        "random_sample_indices = np.random.randint(0, sentence_data.shape[1], size=4)\n",
        "for i, s in enumerate(random_sample_indices):\n",
        "    eos_i = max_sen_len + 2\n",
        "    for idx, token in enumerate(sentence_data[:, s]):\n",
        "        if token == textField.vocab.stoi[\"<eos>\"]:\n",
        "            eos_i = idx\n",
        "            break\n",
        "    sentence = \" \".join([textField.vocab.itos[t] for t in sentence_data[1:eos_i, s]])\n",
        "    print(f\"train sample #{s}: {sentence}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR0tqGJZPXLG"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-4LBbHJPVo4"
      },
      "outputs": [],
      "source": [
        "# Model parameters\n",
        "ntokens = len_vocab\n",
        "e_dim = 512\n",
        "ff_dim = 4 * e_dim\n",
        "nheads = 8\n",
        "nTElayers = 4\n",
        "nTDlayers = 4\n",
        "z_dim = 32\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 30\n",
        "batch_size = 32\n",
        "lr = 0.1\n",
        "milestones = [20, ]\n",
        "# beta related\n",
        "beta_sch = 'anneal' # ['cyclic', 'anneal', 'constant']\n",
        "beta_warmup = 4 # num warmup epochs\n",
        "beta_min = 0.005\n",
        "beta_max = 0.04\n",
        "beta_period = 8 # relevent for 'cyclic' scheduler\n",
        "ratio = 0.75"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVbuel0hct0z"
      },
      "source": [
        "### Model, Loss, Optimizer, Schedulers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loUAfg6Ncuh0"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TransformerVAE(ntokens, e_dim, z_dim, nheads, ff_dim, nTElayers, nTDlayers, pad_idx).to(device)\n",
        "rec_criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)\n",
        "\n",
        "if beta_sch == 'cyclic':\n",
        "    beta_scheduler = cyc_beta_scheduler(epochs=num_epochs, warmup_epochs=beta_warmup, beta_min=beta_min, beta_max=beta_max, period=beta_period, ratio=ratio)\n",
        "elif beta_sch == 'anneal':\n",
        "    beta_scheduler = cyc_beta_scheduler(epochs=num_epochs, warmup_epochs=beta_warmup, beta_min=beta_min, beta_max=beta_max, period=num_epochs-beta_warmup, ratio=ratio)\n",
        "else:  # 'constant'\n",
        "    beta_scheduler = np.concatenate((beta_min * np.ones(beta_warmup), beta_max * np.ones(num_epochs-beta_warmup)))\n",
        "beta = beta_scheduler[0]\n",
        "\n",
        "# print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9YauExSUnp_"
      },
      "outputs": [],
      "source": [
        "# Calculate number of parameters in model\n",
        "dummy_model = TransformerVAE(ntokens, e_dim, z_dim, nheads, ff_dim, nTElayers, nTDlayers, pad_idx)\n",
        "num_trainable_params = sum([p.numel() for p in dummy_model.parameters() if p.requires_grad])\n",
        "print(\"Total number of parameters: \", num_trainable_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3R47sj0XdCbZ"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2YjwmiPdGOz"
      },
      "outputs": [],
      "source": [
        "# Initialize stats and related values\n",
        "rec_loss_log = []\n",
        "kl_loss_log = []\n",
        "bVAE_loss_log = [] # rec_loss + beta * kl_loss\n",
        "log_interval = math.floor((num_sentences / batch_size) / 5)\n",
        "save_interval = 5\n",
        "epoch_model_path = []\n",
        "\n",
        "# Wrap datasets with DataLoader\n",
        "sentences = sentence_data.transpose(0,1)\n",
        "batch_loader = DataLoader(sentences, batch_size=batch_size, shuffle=True)\n",
        "sentences_validation = sentence_data_validation.transpose(0,1)\n",
        "validation_batch_loader = DataLoader(sentences_validation, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    total_rec_loss = 0.\n",
        "    total_kl_loss = 0.\n",
        "    total_bVAE_loss = 0.\n",
        "    total_rec_loss_valid = 0.\n",
        "    total_kl_loss_valid = 0.\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i, batch in enumerate(batch_loader):\n",
        "        batch = batch.transpose(0,1).to(device)\n",
        "        pad_mask, tgt_mask = create_masks(batch, pad_token=pad_idx)\n",
        "        # forward pass\n",
        "        mu, logvar, logits = model(batch, pad_mask, tgt_mask)\n",
        "        # loss calculation\n",
        "        rec_loss = rec_criterion(logits[:-1,:,:].view(-1, ntokens), batch[1:,:].flatten())\n",
        "        kl_loss = calc_kl(mu, logvar)\n",
        "        bVAE_loss = rec_loss + beta * kl_loss\n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        bVAE_loss.backward()\n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # gather and print stats\n",
        "        total_rec_loss  += rec_loss.item()\n",
        "        total_kl_loss   += kl_loss.item()\n",
        "        total_bVAE_loss += bVAE_loss.item()\n",
        "\n",
        "        if epoch == 1 and i == 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.4f} | beta {:02.4f} | ms/batch {:5.2f} | rec_loss {:5.4f} | kl_loss {:5.4f} | bVAE_loss {:5.4f} |'.format(\n",
        "                    epoch, i, len(batch_loader), lr_scheduler.get_last_lr()[0], beta, elapsed * 1000 / log_interval,\n",
        "                    rec_loss, kl_loss, bVAE_loss))\n",
        "        \n",
        "        if i % log_interval == 0 and i > 0:\n",
        "            cur_rec_loss  = total_rec_loss  / log_interval\n",
        "            cur_kl_loss   = total_kl_loss   / log_interval\n",
        "            cur_bVAE_loss = total_bVAE_loss / log_interval\n",
        "\n",
        "            rec_loss_log.append(cur_rec_loss)\n",
        "            kl_loss_log.append(cur_kl_loss)\n",
        "            bVAE_loss_log.append(cur_bVAE_loss)\n",
        "          \n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.4f} | beta {:02.4f} | ms/batch {:5.2f} | rec_loss {:5.4f} | kl_loss {:5.4f} | bVAE_loss {:5.4f} |'.format(\n",
        "                    epoch, i, len(batch_loader), lr_scheduler.get_last_lr()[0], beta, elapsed * 1000 / log_interval, cur_rec_loss, cur_kl_loss, cur_bVAE_loss))\n",
        "            \n",
        "            total_rec_loss = 0.\n",
        "            total_kl_loss = 0.\n",
        "            total_bVAE_loss = 0.\n",
        "            start_time = time.time()\n",
        "\n",
        "    # update learning rate\n",
        "    lr_scheduler.step()\n",
        "    # update beta\n",
        "    if epoch < num_epochs:\n",
        "        beta = beta_scheduler[epoch]\n",
        "\n",
        "    # evaluate model on validation set\n",
        "    if epoch % save_interval == 0 or epoch == num_epochs :\n",
        "        model.eval()\n",
        "        print(\"\\nEvaluating model on validation set...\")\n",
        "        for i, batch in enumerate(validation_batch_loader):\n",
        "            batch = batch.transpose(0,1).to(device)\n",
        "            pad_mask, tgt_mask = create_masks(batch, pad_token=pad_idx)\n",
        "            # forward pass\n",
        "            mu, logvar, logits = model(batch, pad_mask, tgt_mask)\n",
        "            # loss calculation\n",
        "            rec_loss_valid = rec_criterion(logits[:-1,:,:].view(-1, ntokens), batch[1:,:].flatten())\n",
        "            kl_loss_valid = calc_kl(mu, logvar)\n",
        "            total_rec_loss_valid += rec_loss_valid.item()\n",
        "            total_kl_loss_valid  += kl_loss_valid.item()\n",
        "\n",
        "        mean_rec_loss_valid = total_rec_loss_valid / len(validation_batch_loader)\n",
        "        mean_kl_loss_valid = total_kl_loss_valid / len(validation_batch_loader)\n",
        "        perplexity = math.exp(mean_rec_loss_valid)\n",
        "        print('| validation_rec_loss {:5.4f} | validation_kl_loss {:5.4f} | validation_perplexity {:5.2f} |'.format(mean_rec_loss_valid, mean_kl_loss_valid, perplexity))\n",
        "        \n",
        "        total_rec_loss_valid = 0.\n",
        "        total_kl_loss_valid  = 0.\n",
        "        model.train()\n",
        "\n",
        "    # save model\n",
        "    print(\"\\n\")\n",
        "    if epoch % save_interval == 0 or epoch == num_epochs :\n",
        "        print('Saving model ...\\n')\n",
        "        path = './checkpoints/TransformerVAE_{}_epoch_{:3d}_rec_{:5.4f}_kl_{:5.4f}_valid_perp_{:5.2f}.pth'.format(dataset, epoch, cur_rec_loss, cur_kl_loss, perplexity)\n",
        "        if not os.path.isdir('checkpoints'):\n",
        "            os.mkdir('checkpoints')\n",
        "        epoch_model_path.append(path)\n",
        "        torch.save(model.state_dict(), path)\n",
        "\n",
        "print('\\n Finished training')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss curves"
      ],
      "metadata": {
        "id": "0tAL1psdouo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot loss curves\n",
        "fig, axes = plt.subplots(1, 3, figsize=(17,5))\n",
        "axes[0].plot(np.arange(len(rec_loss_log)), rec_loss_log)\n",
        "axes[0].set_title('Reconstruction Loss')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_xlabel(f'Log Interval ({log_interval} batches)')\n",
        "axes[1].plot(np.arange(len(kl_loss_log)), kl_loss_log)\n",
        "axes[1].set_title('KL Loss')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].set_xlabel(f'Log Interval ({log_interval} batches)')\n",
        "axes[2].plot(np.arange(len(bVAE_loss_log)), bVAE_loss_log)\n",
        "axes[2].set_title('beta-VAE Loss')\n",
        "axes[2].set_ylabel('Loss')\n",
        "axes[2].set_xlabel(f'Log Interval ({log_interval} batches)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dWxwiMcTn3qW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trained Model Results"
      ],
      "metadata": {
        "id": "qTgM4k9A8hx_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Model from Checkpoint"
      ],
      "metadata": {
        "id": "9F2DUjYfcZ2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# Choose checkpoint origin: ['drive', 'notebook']\n",
        "origin = 'drive'"
      ],
      "metadata": {
        "id": "ZibHB2tosISh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZY_X5CIgygK"
      },
      "outputs": [],
      "source": [
        "if origin == 'drive':\n",
        "    drive.mount('/content/drive') # give notebook access to google drive to load/save checkpoints\n",
        "    base_path = './drive/' # enter path where checkpoint is saved, example: './drive/checkpoints/'\n",
        "    checkpoint =           # enter checkpoint name, example: 'TransformerVAE_PTB_epoch_ 20_rec_2.5473_kl_7.8348_valid_perp_105.44.pth'\n",
        "    path = base_path + checkpoint\n",
        "\n",
        "elif origin == 'notebook':\n",
        "    path = epoch_model_path[-1] # take last saved checkpoint in training (can choose other)\n",
        "\n",
        "else:\n",
        "    raise NotImplementedError(\"Checkpoint origin is not supported\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load trained model\n",
        "trained_model = TransformerVAE(ntokens, e_dim, z_dim, nheads, ff_dim, nTElayers, nTDlayers, pad_idx).to(device)\n",
        "trained_model.load_state_dict(torch.load(path, map_location=torch.device(device)))\n",
        "trained_model.eval()\n",
        "\n",
        "print(\"Succesfully loaded model from checkpoint\")"
      ],
      "metadata": {
        "id": "46SuZhPepj4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Perplexity"
      ],
      "metadata": {
        "id": "0RXpe92ZvOrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap dataset with DataLoader\n",
        "test_sentences = sentence_data_test.transpose(0,1)\n",
        "test_batch_loader = DataLoader(test_sentences, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Calculate perplexity\n",
        "total_rec_loss = 0.\n",
        "for i, batch in enumerate(test_batch_loader):\n",
        "    batch = batch.transpose(0,1).to(device)\n",
        "    pad_mask, tgt_mask = create_masks(batch, pad_token=pad_idx)\n",
        "    # forward pass\n",
        "    mu, logvar, logits = trained_model(batch, pad_mask, tgt_mask)\n",
        "    # loss calculation\n",
        "    rec_loss = rec_criterion(logits[:-1,:,:].view(-1, ntokens), batch[1:,:].flatten())\n",
        "    total_rec_loss += rec_loss.item()\n",
        "\n",
        "mean_rec_loss = total_rec_loss / len(test_batch_loader)\n",
        "perplexity = math.exp(mean_rec_loss)\n",
        "print('Test perplexity: {:5.2f}'.format(perplexity))"
      ],
      "metadata": {
        "id": "nRYj6sA7vYYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentence Generation"
      ],
      "metadata": {
        "id": "FcBufl7do3AK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0s0Tc48vmM5n"
      },
      "outputs": [],
      "source": [
        "# token_sen, next_word_probs = generate(trained_model, None, z_dim, device, max_sen_len, sos_idx, eos_idx, policy='topK', k=10)\n",
        "token_sen, next_word_probs = trained_model.generate(None, device, max_sen_len, sos_idx, eos_idx, policy='topK', k=3)\n",
        "sentence = \" \".join([textField.vocab.itos[t] for t in token_sen[1:-1]])\n",
        "print(f\"Generated sentence of length {len(token_sen)-2}: {sentence}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot next word probabilities for each word in generated sentence \n",
        "word = []\n",
        "prob = []\n",
        "\n",
        "for i in range(len(token_sen)):\n",
        "  word.append(\" \".join([textField.vocab.itos[t] for t in token_sen[i]]))\n",
        "  if i < len(token_sen)-1:\n",
        "      prob.append(next_word_probs[i].detach().cpu().numpy())\n",
        "\n",
        "prob = np.exp(prob) / np.sum(np.exp(prob)) # softmax\n",
        "\n",
        "fig, axes = plt.subplots(len(token_sen)-1, figsize=(7,5*len(token_sen)))\n",
        "for i in range(len(token_sen)): \n",
        "  if i < len(token_sen)-1:\n",
        "    axes[i].plot(prob[i])\n",
        "    axes[i].set_title('Next Word Probabilities for - \\\"{}\\\"'.format(word[i]))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SGRu7PqqNimv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentence Reconstruction"
      ],
      "metadata": {
        "id": "MeCV4cjIpK39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose source of sentence to reconstruct: ['random', 'validset', 'custom']\n",
        "sen_src = 'random'\n",
        "\n",
        "# if 'validset' choose sample index or None to randomly select an index:\n",
        "sample_idx = None\n",
        "# if 'custom', provide sentence in seperated string list in list format (i.e [[<strings seperated by ,>]]):\n",
        "custom_sen = [['i', 'think', 'therefore', 'i', 'am']]"
      ],
      "metadata": {
        "id": "mZmJqNJJGFWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare sentence\n",
        "if sen_src == 'random': # generate sentence by sampling z ~ N(0,I)\n",
        "    token_sen, _ = trained_model.generate(None, device, max_sen_len, sos_idx, eos_idx, policy='greedy', k=1)\n",
        "    sentence = \" \".join([textField.vocab.itos[t] for t in token_sen[1:-1]])\n",
        "    token_sen = pad_token_sen(token_sen, max_sen_len, pad_idx)\n",
        "\n",
        "elif sen_src == 'validset': # take sentence from training set\n",
        "    if sample_idx is None:\n",
        "        idx = np.random.randint(0, sentence_data_validation.shape[1], size=1)\n",
        "    else:\n",
        "        idx = sample_idx\n",
        "\n",
        "    eos_i = max_sen_len + 2\n",
        "    for i, token in enumerate(sentence_data_validation[:, idx]):\n",
        "        if token == textField.vocab.stoi[\"<eos>\"]:\n",
        "            eos_i = i\n",
        "            break\n",
        "    sentence = \" \".join([textField.vocab.itos[t] for t in sentence_data_validation[1:eos_i, idx]])\n",
        "    token_sen = sentence_data_validation[:, idx]\n",
        "    \n",
        "elif sen_src == 'custom':\n",
        "    sentence = \" \".join(custom_sen[0])\n",
        "    token_sen = textField.pad(custom_sen)\n",
        "    token_sen = textField.numericalize(token_sen)\n",
        "\n",
        "else:\n",
        "    raise NotImplementedError(\"Sentence source is not supported\")\n",
        "\n",
        "# reconstruct\n",
        "token_sen_rec = trained_model.reconstruct(token_sen, device, max_sen_len, pad_idx, sos_idx, eos_idx, policy='greedy', k=1)\n",
        "sentence_reconstructed = \" \".join([textField.vocab.itos[t] for t in token_sen_rec[1:-1]])\n",
        "print('Original sentence:      ', sentence)\n",
        "print('Reconstructed sentence: ', sentence_reconstructed)"
      ],
      "metadata": {
        "id": "SnTaye5doFTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpolation between Sentences in Latent Space"
      ],
      "metadata": {
        "id": "8gOEbcb8pRlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose source of sentences to interpolate between: ['random', 'trainset', 'custom']\n",
        "sen_src_1 = 'trainset'\n",
        "sen_src_2 = 'trainset'\n",
        "\n",
        "# if 'trainset' choose sample index or None to randomly select an index:\n",
        "sample_idx_1 = None\n",
        "sample_idx_2 = None\n",
        "# if 'custom', provide sentence in seperated string list in list format (i.e [[<strings seperated by ,>]]):\n",
        "custom_sen_1 = [['i', 'think', 'therefore', 'i', 'am']]\n",
        "custom_sen_2 = [['all', 'happy', 'families', 'are', 'alike']]"
      ],
      "metadata": {
        "id": "wlRKKn42Wnm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare sentence 1\n",
        "if sen_src_1 == 'random': # generate sentence by sampling z ~ N(0,I)\n",
        "    sen_1 = None\n",
        "\n",
        "elif sen_src_1 == 'trainset': # take sentence from training set\n",
        "    if sample_idx_1 is None:\n",
        "        idx = np.random.randint(0, sentence_data.shape[1], size=1)\n",
        "    else:\n",
        "        idx = sample_idx_1\n",
        "    sen_1 = sentence_data[:, idx]\n",
        "    \n",
        "elif sen_src_1 == 'custom':\n",
        "    sen_1 = textField.pad(custom_sen_1)\n",
        "    sen_1 = textField.numericalize(sen_1)\n",
        "\n",
        "else:\n",
        "    raise NotImplementedError(\"Sentence source is not supported\")\n",
        "\n",
        "# prepare sentence 2\n",
        "if sen_src_2 == 'random': # generate sentence by sampling z ~ N(0,I)\n",
        "    sen_2 = None\n",
        "\n",
        "elif sen_src_2 == 'trainset': # take sentence from training set\n",
        "    if sample_idx_2 is None:\n",
        "        idx = np.random.randint(0, sentence_data.shape[1], size=1)\n",
        "    else:\n",
        "        idx = sample_idx_2\n",
        "    sen_2 = sentence_data[:, idx]\n",
        "    \n",
        "elif sen_src_2 == 'custom':\n",
        "    sen_2 = textField.pad(custom_sen_2)\n",
        "    sen_2 = textField.numericalize(sen_2)\n",
        "\n",
        "else:\n",
        "    raise NotImplementedError(\"Sentence source is not supported\")\n",
        "\n",
        "# interpolate\n",
        "sentences = trained_model.interpolate(sen_1, sen_2, intervals=5, device=device, max_sen_len=max_sen_len, pad_idx=pad_idx, sos_idx=sos_idx, eos_idx=eos_idx, policy='greedy', k=1)\n",
        "for sentence in sentences:\n",
        "    sentence = \" \".join([textField.vocab.itos[t] for t in sentence[1:-1]])\n",
        "    print(sentence)"
      ],
      "metadata": {
        "id": "ZPGEWsFFW2TR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoded Variance Analysis"
      ],
      "metadata": {
        "id": "AhISHZlYKzW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose source of sentence: ['random', 'testset', 'custom']\n",
        "sen_src = 'testset'\n",
        "\n",
        "# if 'testset' choose sample index or None to randomly select an index:\n",
        "sample_idx = None\n",
        "# if 'custom', provide sentence in seperated string list in list format (i.e [[<strings seperated by ,>]]):\n",
        "custom_sen = [['i', 'think', 'therefore', 'i', 'am']]"
      ],
      "metadata": {
        "id": "FXAhaA9nLWml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare sentence\n",
        "if sen_src == 'random': # generate sentence by sampling z ~ N(0,I)\n",
        "    token_sen, _ = trained_model.generate(None, device, max_sen_len, sos_idx, eos_idx, policy='greedy', k=1)\n",
        "    sentence = \" \".join([textField.vocab.itos[t] for t in token_sen[1:-1]])\n",
        "    word_sen = [textField.vocab.itos[t] for t in token_sen[1:-1]]\n",
        "    token_sen = pad_token_sen(token_sen, max_sen_len, pad_idx)\n",
        "\n",
        "    shuffled_sen = random.sample(word_sen, len(word_sen))\n",
        "    shuffled_token_sen = textField.pad(shuffled_sen)\n",
        "    shuffled_token_sen = textField.numericalize(shuffled_token_sen)\n",
        "    shuffled_sentence = \" \".join(shuffled_sen)\n",
        "\n",
        "elif sen_src == 'testset': # take sentence from training set\n",
        "    if sample_idx is None:\n",
        "        idx = np.random.randint(0, sentence_data_test.shape[1], size=1)\n",
        "    else:\n",
        "        idx = sample_idx\n",
        "\n",
        "    eos_i = max_sen_len + 2\n",
        "    for i, token in enumerate(sentence_data_test[:, idx]):\n",
        "        if token == textField.vocab.stoi[\"<eos>\"]:\n",
        "            eos_i = i\n",
        "            break\n",
        "    sentence = \" \".join([textField.vocab.itos[t] for t in sentence_data_test[1:eos_i, idx]])\n",
        "    token_sen = sentence_data_test[:, idx].to(device)\n",
        "\n",
        "    word_sen = [textField.vocab.itos[t] for t in sentence_data_test[1:eos_i, idx]]\n",
        "    shuffled_sen = random.sample(word_sen, len(word_sen))\n",
        "    shuffled_token_sen = textField.pad(shuffled_sen)\n",
        "    shuffled_token_sen = textField.numericalize(shuffled_token_sen).to(device)\n",
        "    shuffled_sentence = \" \".join(shuffled_sen)\n",
        "    \n",
        "elif sen_src == 'custom':\n",
        "    sentence = \" \".join(custom_sen[0])\n",
        "    token_sen = textField.pad(custom_sen)\n",
        "    token_sen = textField.numericalize(token_sen).to(device)\n",
        "\n",
        "    shuffled_sen = random.sample(custom_sen[0], len(custom_sen[0]))\n",
        "    shuffled_token_sen = textField.pad(shuffled_sen)\n",
        "    shuffled_token_sen = textField.numericalize(shuffled_token_sen).to(device)\n",
        "    shuffled_sentence = \" \".join(shuffled_sen)\n",
        "\n",
        "else:\n",
        "    raise NotImplementedError(\"Sentence source is not supported\")\n",
        "\n",
        "\n",
        "# Calculate variance of latent representation\n",
        "# Note: larger variance -> the model is less sure about the encoding\n",
        "#       we expect the variance in the shuffled sentence to be larger if the\n",
        "#       model learned sentence structure well\n",
        "pad_mask = (token_sen == pad_idx).transpose(0, 1)\n",
        "_, logvar = trained_model.encoder(token_sen, pad_mask)\n",
        "var = torch.exp(logvar)\n",
        "print('Original sentence:      ', sentence)\n",
        "print(f\"Variance mean across latent dimensions: {torch.mean(var)}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "pad_mask = (shuffled_token_sen == pad_idx).transpose(0, 1)\n",
        "_, logvar = trained_model.encoder(shuffled_token_sen, pad_mask)\n",
        "var = torch.exp(logvar)\n",
        "print('Shuffled sentence:      ', shuffled_sentence)\n",
        "print(f\"Variance mean across latent dimensions: {torch.mean(var)}\")"
      ],
      "metadata": {
        "id": "yBmpeYp2LIkK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "TransformerVAE.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}